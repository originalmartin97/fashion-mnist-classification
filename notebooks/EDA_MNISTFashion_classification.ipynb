{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e0dfd4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a162a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch library - the main framework for building neural networks\n",
    "import torch\n",
    "import torch.nn as nn  # nn = neural network building blocks\n",
    "import torch.optim as optim  # Optimization algorithms (the \"teacher\" that corrects mistakes)\n",
    "\n",
    "\n",
    "# For loading datasets and transforming images\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For visualizing our data and results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For numerical operations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f77b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're using CPU or GPU (in your case, it'll be CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility (so results are consistent each time)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68d7d7",
   "metadata": {},
   "source": [
    "## Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d23f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download the dataset WITHOUT any transformations first\n",
    "# We only convert to tensor (no normalization yet) so we can see raw pixel values\n",
    "raw_transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Just convert to tensor, keep original 0-1 range\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "# train dataset\n",
    "train_dataset_raw = torchvision.datasets.FashionMNIST(\n",
    "    root='../data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=raw_transform\n",
    ")\n",
    "# test dataset\n",
    "test_dataset_raw = torchvision.datasets.FashionMNIST(\n",
    "    root='../data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=raw_transform\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded successfully!\")\n",
    "print(f\"{train_dataset_raw.data.shape}\")\n",
    "print(f\"{test_dataset_raw.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2422d",
   "metadata": {},
   "source": [
    "## Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ec253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC INFORMATION\n",
    "\n",
    "# What we're checking:\n",
    "# - How many images do we have?\n",
    "# - What size are the images? (28√ó28 pixels)\n",
    "# - What format is the data in? (tensors)\n",
    "# - How many categories? (10 clothing types)\n",
    "\n",
    "# Label = shows the real type and value of the data\n",
    "# (in the dataset as how they are stored - not like this is an image or an ankle boot)\n",
    "\n",
    "# Label Number  ‚Üí  Clothing Type\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 0             ‚Üí  T-shirt/top\n",
    "# 1             ‚Üí  Trouser\n",
    "# 2             ‚Üí  Pullover\n",
    "# 3             ‚Üí  Dress\n",
    "# 4             ‚Üí  Coat\n",
    "# 5             ‚Üí  Sandal\n",
    "# 6             ‚Üí  Shirt\n",
    "# 7             ‚Üí  Sneaker\n",
    "# 8             ‚Üí  Bag\n",
    "# 9             ‚Üí  Ankle boot\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Training samples: {len(train_dataset_raw):,}\")\n",
    "print(f\"Test samples: {len(test_dataset_raw):,}\")\n",
    "print(f\"Total samples: {len(train_dataset_raw) + len(test_dataset_raw):,}\")\n",
    "\n",
    "# Get one sample to inspect\n",
    "sample_image, sample_label = train_dataset_raw[0]\n",
    "print(f\"\\nImage shape: {sample_image.shape}\")  # Should be [1, 28, 28]\n",
    "print(f\"Image data type: {sample_image.dtype}\")\n",
    "print(f\"Label type: {type(sample_label)}\")\n",
    "print(f\"Label value: {sample_label}\")\n",
    "\n",
    "# Class names\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "print(f\"\\nNumber of classes: {len(classes)}\")\n",
    "print(f\"Class names: {classes}\")\n",
    "\n",
    "# Results show that we have a total of 70 000 images to work with.\n",
    "# The images one by one are only 2D with the channel of grayscale hence the dimensions [1, 28, 28],\n",
    "# which translates to 28 by 28 pixel size by each individual image.\n",
    "# There are 10 categories corresponding with the dataset indices of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c80ea",
   "metadata": {},
   "source": [
    "## Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b90a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SAMPLES FOR REASSURANCE\n",
    "\n",
    "# What we're checking:\n",
    "# - What does each clothing type actually look like?\n",
    "# - Are the images clear enough?\n",
    "# - Do the labels make sense?\n",
    "\n",
    "\n",
    "# Let's look at random samples from each class\n",
    "print(\"\\nüñºÔ∏è  SAMPLE IMAGES FROM EACH CLASS:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('One Sample from Each Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, class_name in enumerate(classes):\n",
    "    # Find first image of this class\n",
    "    for i in range(len(train_dataset_raw)):\n",
    "        img, label = train_dataset_raw[i]\n",
    "        if label == idx:\n",
    "            # Plot it\n",
    "            row = idx // 5\n",
    "            col = idx % 5\n",
    "            axes[row, col].imshow(img.squeeze(), cmap='gray')\n",
    "            axes[row, col].set_title(f'{idx}: {class_name}')\n",
    "            axes[row, col].axis('off')\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Result show all the images are greyscaled and based on black background and the labels make sense.\n",
    "# As for the image quality in my opinion it is totally okay as we can tell them apart and define what\n",
    "# we are seeing (no ambiguity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23011676",
   "metadata": {},
   "source": [
    "## Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b53382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS DISTRIBUTION FOR BALANCE CHECK\n",
    "\n",
    "# What we're checking:\n",
    "# - Are all classes equally represented? (balanced dataset?)\n",
    "# - If one class has way more examples, the model might be biased towards it\n",
    "# - Fashion-MNIST is balanced (6000 per class in training), but good to verify!\n",
    "\n",
    "print(\"\\nüìà CLASS DISTRIBUTION ANALYSIS:\")\n",
    "\n",
    "# Count how many images per class\n",
    "train_labels = [label for _, label in train_dataset_raw]\n",
    "test_labels = [label for _, label in test_dataset_raw]\n",
    "\n",
    "# Count occurrences\n",
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(train_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "# Print table\n",
    "print(\"\\nTraining Set Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Class':<15} {'Name':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for class_idx in range(10):\n",
    "    count = train_counts[class_idx]\n",
    "    percentage = (count / len(train_dataset_raw)) * 100\n",
    "    print(f\"{class_idx:<15} {classes[class_idx]:<15} {count:<10} {percentage:.1f}%\")\n",
    "\n",
    "print(\"\\nTest Set Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Class':<15} {'Name':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for class_idx in range(10):\n",
    "    count = test_counts[class_idx]\n",
    "    percentage = (count / len(test_dataset_raw)) * 100\n",
    "    print(f\"{class_idx:<15} {classes[class_idx]:<15} {count:<10} {percentage:.1f}%\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training distribution\n",
    "ax1.bar(range(10), [train_counts[i] for i in range(10)], color='steelblue')\n",
    "ax1.set_xlabel('Class Index')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.set_title('Training Set - Class Distribution')\n",
    "ax1.set_xticks(range(10))\n",
    "ax1.set_xticklabels([f'{i}\\n{classes[i][:8]}' for i in range(10)], rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Test distribution\n",
    "ax2.bar(range(10), [test_counts[i] for i in range(10)], color='coral')\n",
    "ax2.set_xlabel('Class Index')\n",
    "ax2.set_ylabel('Number of Images')\n",
    "ax2.set_title('Test Set - Class Distribution')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_xticklabels([f'{i}\\n{classes[i][:8]}' for i in range(10)], rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Results show we are good as there is not a single problem with the distribution of the data\n",
    "# (nothing missing or lacking in amount for all the labels, no possible bias towards any class - same values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a41737",
   "metadata": {},
   "source": [
    "## Image Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a303880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY IMAGE CONSISTENCY\n",
    "\n",
    "# What we're checking:\n",
    "# - Are all images the same size? (important for neural networks!)\n",
    "# - Grayscale (1 channel) or color (3 channels)?\n",
    "\n",
    "print(\"\\n‚úÖ IMAGE CONSISTENCY CHECK:\")\n",
    "\n",
    "# Check if all images have the same dimensions\n",
    "all_same = True\n",
    "expected_shape = (1, 28, 28)\n",
    "\n",
    "for i in range(min(100, len(train_dataset_raw))):  # Check first 100\n",
    "    img, _ = train_dataset_raw[i]\n",
    "    if img.shape != expected_shape:\n",
    "        print(f\"‚ùå Image {i} has unexpected shape: {img.shape}\")\n",
    "        all_same = False\n",
    "        break\n",
    "\n",
    "if all_same:\n",
    "    print(f\"‚úÖ All images have consistent shape: {expected_shape}\")\n",
    "    print(f\"   - Channels: {expected_shape[0]}\")\n",
    "    print(f\"   - Height: {expected_shape[1]} pixels\")\n",
    "    print(f\"   - Width: {expected_shape[2]} pixels\")\n",
    "    print(f\"   - Total pixels per image: {28*28} should equal to 784\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5a421",
   "metadata": {},
   "source": [
    "## Pixel Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12383ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIXEL VALUE ANALYSIS\n",
    "\n",
    "# What we're checking:\n",
    "# - What range are pixel values in? (0-1 range)\n",
    "# - What's the average brightness?\n",
    "# - Are most pixels dark or light?\n",
    "# - This tells us HOW to normalize in the next section!\n",
    "\n",
    "print(\"\\nüî¢ PIXEL VALUE STATISTICS:\")\n",
    "\n",
    "# Sample 1000 random images to analyze\n",
    "sample_size = 1000\n",
    "sample_indices = np.random.choice(len(train_dataset_raw), sample_size, replace=False)\n",
    "\n",
    "pixel_values = []\n",
    "for idx in sample_indices:\n",
    "    img, _ = train_dataset_raw[idx]\n",
    "    pixel_values.extend(img.flatten().numpy()) # flatten?\n",
    "\n",
    "pixel_values = np.array(pixel_values)\n",
    "\n",
    "print(f\"\\nAnalyzed {sample_size} random images\")\n",
    "print(f\"Total pixels analyzed: {len(pixel_values):,}\")\n",
    "print(f\"Supposed amount of analysed pixels: {1000*(28*28)}\") # intentionally hard-coded\n",
    "print(f\"Min pixel value: {pixel_values.min():.4f}\")\n",
    "print(f\"Max pixel value: {pixel_values.max():.4f}\")\n",
    "print(f\"Mean pixel value: {pixel_values.mean():.4f}\")\n",
    "print(f\"Std deviation: {pixel_values.std():.4f}\")\n",
    "print(f\"Median: {np.median(pixel_values):.4f}\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(pixel_values, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Pixel Value (0-1 range)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Pixel Values (Sample of 1000 Images)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Results show most pixel values are close to 0 - logical cause we have grayscale images with based black background.\n",
    "# The correct amount of analyzed pixels should be counted by the following mathematical logic:\n",
    "# We get a 1000 image and each image is a 28 by 28 pixel thus amount_of_images * (each_individual_image_total_pixel_size)\n",
    "# The average pixel brightness is the \"Mean pixel value\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66953ded",
   "metadata": {},
   "source": [
    "## EDA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training samples: {len(train_dataset_raw):,}\")\n",
    "print(f\"Test samples: {len(test_dataset_raw):,}\")\n",
    "print(f\"Total samples: {len(train_dataset_raw) + len(test_dataset_raw):,}\")\n",
    "print(f\"‚úÖ All images have consistent shape: {expected_shape}\")\n",
    "print(f\"   - Channels: {expected_shape[0]}\")\n",
    "print(f\"   - Height: {expected_shape[1]} pixels\")\n",
    "print(f\"   - Width: {expected_shape[2]} pixels\")\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "print(f\"Class names: {classes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
