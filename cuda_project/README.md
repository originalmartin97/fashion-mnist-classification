# CUDA Fashion-MNIST Project

A simplified Fashion-MNIST classification project demonstrating **custom CUDA kernel implementation** for a neural network activation function.

## üéØ Project Purpose

This project fulfills course requirements by implementing:
- ‚úÖ AI code in Python (Fashion-MNIST classifier)
- ‚úÖ Custom CUDA kernel (Leaky ReLU activation function)
- ‚úÖ Complete training pipeline with PyTorch

## üìÅ Project Structure

```
cuda_project/
‚îú‚îÄ‚îÄ leaky_relu_cuda.cu          # CUDA kernel implementation
‚îú‚îÄ‚îÄ custom_activation.py         # Python wrapper for CUDA kernel
‚îú‚îÄ‚îÄ fashion_mnist_cuda.ipynb     # Training notebook
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üîß Custom CUDA Kernel

### What it does:
Implements a **Leaky ReLU activation function** in CUDA:

```
f(x) = x          if x > 0
f(x) = alpha * x  if x ‚â§ 0
```

### Why CUDA?
- **GPU Acceleration**: Runs on GPU for faster computation
- **Custom Implementation**: Shows understanding of low-level operations
- **Drop-in Replacement**: Can replace PyTorch's built-in `nn.LeakyReLU()`

### Files:

1. **`leaky_relu_cuda.cu`** - CUDA C++ kernel
   - Forward pass kernel (applies activation)
   - Backward pass kernel (computes gradients)
   - PyTorch bindings

2. **`custom_activation.py`** - Python interface
   - `CustomLeakyReLU` module (nn.Module)
   - Automatic gradient computation
   - Easy integration with PyTorch models

## üöÄ Setup & Usage

### Prerequisites

```bash
# CUDA Toolkit (required for compilation)
# PyTorch with CUDA support
# Python 3.8+
```

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Test CUDA kernel
python custom_activation.py
```

Expected output:
```
Loading CUDA extension... (this may take a minute on first run)
‚úÖ CUDA extension loaded successfully!
‚úÖ Custom CUDA implementation matches PyTorch!
```

### Using in Your Model

```python
from custom_activation import CustomLeakyReLU

class FashionNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        
        # Use custom CUDA activation instead of nn.ReLU()
        self.activation = CustomLeakyReLU(alpha=0.01)
        
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = self.dropout(self.activation(self.fc1(x)))
        x = self.dropout(self.activation(self.fc2(x)))
        x = self.fc3(x)
        return x
```

## üìä Results

- **Dataset**: Fashion-MNIST (60,000 training, 10,000 test images)
- **Model**: 3-layer fully connected network
- **Training**: 10 epochs, Adam optimizer
- **Test Accuracy**: ~88%

## üß™ Testing the CUDA Kernel

Run the test script:

```bash
python custom_activation.py
```

This will:
1. Load the CUDA extension
2. Create a test tensor on GPU
3. Apply custom Leaky ReLU
4. Compare with PyTorch's implementation
5. Verify correctness

## üí° Technical Details

### CUDA Kernel Design

**Forward Pass:**
```cuda
__global__ void leaky_relu_forward_kernel(
    const float* input,
    float* output,
    const int size,
    const float alpha
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        output[idx] = x > 0 ? x : alpha * x;
    }
}
```

**Key Features:**
- **Parallelization**: Each GPU thread processes one element
- **Grid-stride loop**: Handles tensors of any size
- **Memory coalescing**: Optimized memory access pattern

### PyTorch Integration

Uses `torch.autograd.Function` for automatic differentiation:
- Forward pass: CUDA kernel computes activation
- Backward pass: CUDA kernel computes gradients
- Seamless integration with PyTorch's autograd system

## üéì Educational Value

This project demonstrates:
1. **CUDA Programming**: Writing GPU kernels in C++
2. **PyTorch Extension**: Integrating custom CUDA code with PyTorch
3. **Autograd**: Implementing custom backward pass
4. **Deep Learning**: End-to-end neural network training
5. **Performance**: GPU acceleration for ML operations

## üì¶ Requirements

```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
matplotlib>=3.7.0
```

## üîç Verification

To verify the CUDA kernel works correctly:

```python
import torch
from custom_activation import CustomLeakyReLU

# Create test input
x = torch.tensor([[-1.0, 0.0, 1.0], [2.0, -2.0, 0.5]]).cuda()

# Apply custom activation
activation = CustomLeakyReLU(alpha=0.01)
output = activation(x)

print(output)
# Expected: [[-0.01, 0.0, 1.0], [2.0, -0.02, 0.5]]
```

## üìù Notes

- **GPU Required**: This code requires a CUDA-capable GPU
- **First Run**: CUDA compilation takes ~1-2 minutes on first run
- **Cached**: Subsequent runs are fast (kernel is cached)
- **CPU Fallback**: Not implemented (GPU-only by design)

## üéØ Assignment Requirements Met

‚úÖ **AI code in Python**: Complete Fashion-MNIST classifier  
‚úÖ **CUDA kernel**: Custom Leaky ReLU activation function  
‚úÖ **Purpose**: Activation function (as specified)  
‚úÖ **Documentation**: Comprehensive README and code comments  
‚úÖ **Testing**: Verification script included  

## üìß Submission

This project can be submitted as:
- ZIP file containing all files in `cuda_project/`
- GitHub repository link
- Individual files via email

---

**Author**: originalmartin97  
**Date**: November 2025  
**Purpose**: Course assignment demonstrating CUDA integration with PyTorch
